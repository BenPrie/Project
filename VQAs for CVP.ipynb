{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T18:28:34.893739800Z",
     "start_time": "2024-01-13T18:28:34.806468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports, as always...\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "\n",
    "# For generating prime numbers.\n",
    "from Crypto.Util import number\n",
    "\n",
    "# Notebook progress bars.\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sqif import CVP, solve_cvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def give_me_a_blank_results_dataframe():\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            'Bit-length' : [],\n",
    "            'Lattice Dimension' : [],\n",
    "            'N' : [],\n",
    "            'l' : [],\n",
    "            'c' : [],\n",
    "            '|b_op - t|^2' : [],\n",
    "            'P(b_op)' : [],\n",
    "            '|v_best - t|^2' : [],\n",
    "            'P(v_best)' : [],\n",
    "            'P(|v_new - t|^2 < |b_op - t|^2)' : [],\n",
    "            'P(|v_new - t|^2 > |b_op - t|^2)' : [],\n",
    "            'E[|v_new - t|^2]' : []\n",
    "        }\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T18:20:39.928684400Z",
     "start_time": "2024-01-13T18:20:39.919174700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQAs for CVP\n",
    "\n",
    "A very important part of Yan et al. (2022)'s proposal for a quantum-accelerated variant of Schnorr's classical factoring algorithm is that an approximate solution to the closest vector problem (CVP) could be improved upon by considering a superposition over the states forming the unit hypercube around that solution, then using a variational algorithm (they use QAOA) to sample higher quality solutions from that search space.\n",
    "\n",
    "The relevance of this within the picture of factoring is that we have, by Schnorr's classical factoring method, a sieve-based factoring method requiring sufficiently many \"smooth-relation pairs (sr-pairs)\" which we may found by reducing the problem to a closest vector problem (CVP) on the prime lattice whose elemental points map to sr-pairs. The higher the quality of our solution to the CVP, the 'better' our sr-pairs are more likely to be, and so we require fewer of them to form a suitable system of equations whose solution yields (part of) a solution to the original factorisation. \n",
    "\n",
    "Of course, Schnorr's algorithm is deeply flawed, but that is besides the point -- we know in what ways it is flawed, and why, thanks to works such as LÃ©o Ducas' excellent repository. Our concern, in this notebook, is that *throwing the thing into a variational algorithm is not the silver bullet*; we suggest that Yan et al. (2022)'s proposal is built on a weak claim about lattice dimension, and so we do not expect better results even when using a quantum algorithm to improve the solution to the CVP. Indeed, we doubt their methodology is sophisticated enough to even improve the CVP effectively as the lattice dimension increases, particularly as the lattice dimension is given to scale \"sublinearly\" based on claims that are not expected to hold.\n",
    "\n",
    "So, in summary, the purpose of this notebook is to empirically demonstrate that their variational approach, with a Hamiltonian mapped as they describe, offers an insufficient scaling with respect to the probability to sample better solutions to the CVP and lattice dimension, and thus their approach would not scale well even when issues with Schnorr's classical approach are not fundamental (which, unfortunately, they are).\n",
    "\n",
    "**Hopefully, we can also try out a few other quantum heuristics (e.g. VQE, AQC-PQC), and maybe even a brute-force search of the entire hyper-cube to demonstrate that the idea of 'searching around the approximate solution on the off chance a rounding operation was not so good' is wholy insufficient to solve a problem as difficult as prime factorisation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# The expected quality of the solution.\n",
    "def expected_dist(solutions, probabilities, t):\n",
    "    # Compute each solution's distance to the target.\n",
    "    compute_dist = lambda x : np.linalg.norm(x - t)\n",
    "    dists = np.apply_along_axis(compute_dist, 1, solutions)\n",
    "    \n",
    "    # Take a weighted average over these distances (weighted by their probabilities).\n",
    "    return np.average(dists, weights=probabilities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T13:14:56.870346300Z",
     "start_time": "2024-01-13T13:14:56.858824500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# The probability to measure the best state...\n",
    "def report_algorithm_effectiveness(solutions, probabilities, approximate_solution, t):\n",
    "    approximate_quality = np.linalg.norm(approximate_solution - t)\n",
    "    print(f'Our approximate solution by Babai\\'s algorithm has a distance to t of {round(approximate_quality, 3):.3f}.\\n')\n",
    "    \n",
    "    # First, determine which is the best state! This computation will scale exponentially with problem size, so let's not do this in practice -- just for analysis every once in a while.\n",
    "    best_dist_to_target = np.inf\n",
    "    best_prob = 0\n",
    "    for v_new, prob in zip(solutions, probabilities):\n",
    "        dist_to_target = np.linalg.norm(v_new - t)\n",
    "        \n",
    "        if dist_to_target < best_dist_to_target:\n",
    "            best_dist_to_target = dist_to_target\n",
    "            best_prob = prob\n",
    "            \n",
    "    print(f'The closest we get to t is {round(best_dist_to_target, 3):.3f}, which we obtain with a probability {round(best_prob, 3):.3f}.\\n')\n",
    "    \n",
    "    # The cumulative probability to measure a state corresponding to a solution that is a least as good as b_op...\n",
    "    better_cum_prob = 0\n",
    "    worse_cum_prob = 0\n",
    "    for v_new, prob in zip(solutions, probabilities):\n",
    "        dist_to_target = np.linalg.norm(v_new - t)\n",
    "        \n",
    "        if dist_to_target < approximate_quality:\n",
    "            better_cum_prob += prob\n",
    "        elif dist_to_target > approximate_quality:\n",
    "            worse_cum_prob += prob\n",
    "            \n",
    "    print(f'The probability to obtain a state vector corresponding to a solution that is BETTER than the classically-obtained approximate solution is {round(better_cum_prob, 3):.3f}.')\n",
    "    print(f'The probability to obtain a state vector corresponding to a solution that is WORSE than the classically-obtained approximate solution is {round(worse_cum_prob, 3):.3f}.\\n')\n",
    "    \n",
    "    print(f'The EXPECTED distance to t is {expected_dist(solutions, probabilities, cvp.t):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T13:14:59.646591100Z",
     "start_time": "2024-01-13T13:14:59.598017700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T13:04:10.742514200Z",
     "start_time": "2024-01-13T13:04:08.426827400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer to factor: 169183578713153\n",
      "Lattice dimension: 8\n",
      "\n",
      "...\n",
      "Our approximate solution by Babai's algorithm has a distance to t of 13.115.\n",
      "\n",
      "The closest we get to t is 13.115, which we obtain with a probability 0.022.\n",
      "\n",
      "The probability to obtain a state vector corresponding to a solution that is BETTER than the classically-obtained approximate solution is 0.000.\n",
      "The probability to obtain a state vector corresponding to a solution that is WORSE than the classically-obtained approximate solution is 0.978.\n",
      "\n",
      "The EXPECTED distance to t is 16.082\n"
     ]
    }
   ],
   "source": [
    "# The integer bit-length we'd like to solve for.\n",
    "n_bits = 48\n",
    "\n",
    "# Lattice and precision parameters.\n",
    "l = 1\n",
    "c = 4\n",
    "\n",
    "# Generating a prime number for which the above lattice dimension is required.\n",
    "N = number.getPrime(n_bits)\n",
    "#N=1961\n",
    "print(f'Integer to factor: {N}')\n",
    "\n",
    "# Set up the CVP.\n",
    "cvp = CVP()\n",
    "cvp.generate_cvp(N, l=l, c=c, seed=42)\n",
    "print(f'Lattice dimension: {cvp.m}')\n",
    "\n",
    "print('\\n...')\n",
    "\n",
    "# Solve it!\n",
    "solutions, probabilities, approximate_solution = solve_cvp(cvp, n_samples=1000, delta=.75, p=1, min_method='Nelder-Mead', verbose=False)\n",
    "report_algorithm_effectiveness(solutions, probabilities, approximate_solution, cvp.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   Bit-length  Lattice Dimension     N  l    c  |b_op - t|^2  P(b_op)  \\\n0          11                  3  1471  1  1.5      5.477226    0.491   \n\n   |v_best - t|^2  P(v_best)  P(|v_new - t|^2 < |b_op - t|^2)  \\\n0        5.477226      0.491                                0   \n\n   P(|v_new - t|^2 > |b_op - t|^2)  E[|v_new - t|^2]  \n0                            0.509          5.828433  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bit-length</th>\n      <th>Lattice Dimension</th>\n      <th>N</th>\n      <th>l</th>\n      <th>c</th>\n      <th>|b_op - t|^2</th>\n      <th>P(b_op)</th>\n      <th>|v_best - t|^2</th>\n      <th>P(v_best)</th>\n      <th>P(|v_new - t|^2 &lt; |b_op - t|^2)</th>\n      <th>P(|v_new - t|^2 &gt; |b_op - t|^2)</th>\n      <th>E[|v_new - t|^2]</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11</td>\n      <td>3</td>\n      <td>1471</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>5.477226</td>\n      <td>0.491</td>\n      <td>5.477226</td>\n      <td>0.491</td>\n      <td>0</td>\n      <td>0.509</td>\n      <td>5.828433</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From-scratch QAOA-based \"SQIF\" algorithm on the generated CVP.\n",
    "def from_scratch_qaoa_experiment(n_bits, l, c, seed=42, n_samples=1000, delta=.75, p=1):\n",
    "    N = number.getPrime(n_bits)\n",
    "    \n",
    "    # Set up the CVP.\n",
    "    cvp = CVP()\n",
    "    cvp.generate_cvp(N, l=l, c=c, seed=seed)\n",
    "    \n",
    "    # Solve it!\n",
    "    solutions, probabilities, approximate_solution = solve_cvp(cvp, n_samples=n_samples, delta=delta, p=p, min_method='Nelder-Mead', verbose=False)\n",
    "    \n",
    "    approximate_quality = np.linalg.norm(approximate_solution - cvp.t)\n",
    "    \n",
    "    # Determine the quality of the SQIF algorithm's solutions to the CVP (in terms of probability).\n",
    "    \n",
    "    # First, determine which is the best state! This computation will scale exponentially with problem size, so let's not do this in practice -- just for analysis every once in a while.\n",
    "    best_dist_to_target = np.inf\n",
    "    best_prob = 0\n",
    "    for v_new, prob in zip(solutions, probabilities):\n",
    "        dist_to_target = np.linalg.norm(v_new - cvp.t)\n",
    "        \n",
    "        if dist_to_target < best_dist_to_target:\n",
    "            best_dist_to_target = dist_to_target\n",
    "            best_prob = prob\n",
    "    \n",
    "    # The cumulative probability to measure a state corresponding to a solution that is a least as good as b_op...\n",
    "    better_cum_prob = 0\n",
    "    worse_cum_prob = 0\n",
    "    for v_new, prob in zip(solutions, probabilities):\n",
    "        dist_to_target = np.linalg.norm(v_new - cvp.t)\n",
    "        \n",
    "        if dist_to_target < approximate_quality:\n",
    "            better_cum_prob += prob\n",
    "        elif dist_to_target > approximate_quality:\n",
    "            worse_cum_prob += prob\n",
    "            \n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            'Bit-length' : [N.bit_length()],\n",
    "            'Lattice Dimension' : [cvp.m],\n",
    "            'N' : [N],\n",
    "            'l' : [l],\n",
    "            'c' : [c],\n",
    "            '|b_op - t|^2' : [approximate_quality],\n",
    "            'P(b_op)' : [1 - (better_cum_prob + worse_cum_prob)],\n",
    "            '|v_best - t|^2' : [best_dist_to_target],\n",
    "            'P(v_best)' : [best_prob],\n",
    "            'P(|v_new - t|^2 < |b_op - t|^2)' : [better_cum_prob],\n",
    "            'P(|v_new - t|^2 > |b_op - t|^2)' : [worse_cum_prob],\n",
    "            'E[|v_new - t|^2]' : [expected_dist(solutions, probabilities, cvp.t)]\n",
    "        }\n",
    "    )\n",
    "\n",
    "from_scratch_qaoa_experiment(11, l=1, c=1.5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T13:15:06.597339400Z",
     "start_time": "2024-01-13T13:15:06.035327300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QAOA-based, Independent-training\n",
    "\n",
    "The SQIF algorithm using a QAOA-based quantum \"speed-up\" mechanism, independently training and running in each input semi-prime. This is taxing on resources, and only really gives us useful insight into the algorithm as applied as a framework requiring a full training setup for every input, but it is necessary to understand a broad range of input independently under the same framework."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d22e270f2b69492cb134276dc69e2777"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For reference, with n_bits in [5, 50] and 5 repeats for each, p=1 takes ~5 min, p=2 takes ~30 min, p=3 takes ~75 min.\n",
    "\n",
    "# Number of QAOA layers.\n",
    "for p in tqdm(range(1, 4)):\n",
    "    results = give_me_a_blank_results_dataframe()\n",
    "    \n",
    "    # Bit-length for the input semi-prime.\n",
    "    for n_bits in range(5, 51):\n",
    "        # How many repeats for each bit-length.\n",
    "        for _ in range(5):\n",
    "            experiment_outcome = from_scratch_qaoa_experiment(n_bits, l=1, c=1.5, seed=42, p=p)\n",
    "            results = pd.concat([results, experiment_outcome])\n",
    "            \n",
    "    results.to_csv(f'./results/quantum-accelerated-cvp/qaoa-based-(p={p})-independent-training.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T20:22:38.556012Z",
     "start_time": "2024-01-13T18:31:37.525520600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QAOA-based, Independent-training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
